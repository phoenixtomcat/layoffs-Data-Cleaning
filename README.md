# layoffs-Data-Cleaning

- Imported and structured a raw dataset of 2,361 records, addressing issues such as duplicates, inconsistent data formats, and null values.

- Identified and removed duplicates using advanced SQL techniques, ensuring data integrity by partitioning and filtering based on unique attributes.

- Standardized inconsistent data, including correcting misspellings, trimming white spaces, and normalizing industry categories (e.g., merging "Crypto" and "Cryptocurrency").

- Addressed null and blank values through self-joins, updating incomplete records with relevant data from existing rows.

- Converted textual date formats to proper date data types, enabling seamless time-series analysis.

- Established a raw and staging table workflow to preserve original data integrity while allowing iterative transformations on staging tables.

- Removed irrelevant rows and redundant columns to optimize storage and performance for downstream analysis.


